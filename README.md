# Audio Segmenter

Inspired by [GroupVit](https://arxiv.org/abs/2202.11094), this repository contains code
that tries to adapt the architecture for audio segmentation. Concisely, given an audio
sample and a word uttered within it, this model is supposed to encode in the grouping
blocks's attention matrices the mask that points to where that word is present within
the audio sample.

To train the model, first enter your WANDB API key in the `config/config.py` file:

```
WANDB_API_KEY = "<your_api_key>"
```

Then, download the LibriSpeech training dataset:

```
python download_dataset.py
```

Then, just run `train.py`:

```
python train.py
```

You can configure some of the configuration variables in the `config/config.py` file.

## Loss construction

The loss computation is divided into two steps. The first step computes a cosine
similarity metric based on the audio and text embeddings. The second computes a word-level
cosine-similarity metric, to further finegrain the contrastive embeddings generated by
the audio model.

## Word-Level Loss

Let $A$ be the matrix such that entry $A_{ij}$ denotes the cosine similarity between audio sample $i$ and individual word $j$ in the batch.

Let $\hat{1}$ denote the matrix whose entry $\hat{1}_{ij}$ is $1$ if word $j$ is in audio sample $i$, and $0$ otherwise.

Then, the word-level loss is defined as the binary cross-entropy loss for words that are present in each audio and words that are not:

```math
L = \sum_{i} \sum_{j} \hat{1}_{ij} \cdot log(A_{ij}) + (1 - \hat{1}_{ij}) \cdot log(1 - A_{ij})
```
